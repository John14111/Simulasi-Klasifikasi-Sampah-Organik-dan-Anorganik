{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d4443-5b84-41b7-879f-ccec6d1bcbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Ensure GPU is used if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Paths to dataset\n",
    "train_dir = r'C:/Users/asus/Downloads/Testing Simulasi Klasifikasi Sampah/Dataset sampah sendiri/garbage_classification_sendiri/train'\n",
    "valid_dir = r'C:/Users/asus/Downloads/Testing Simulasi Klasifikasi Sampah/Dataset sampah sendiri/garbage_classification_sendiri/validation'\n",
    "test_dir = r'C:/Users/asus/Downloads/Testing Simulasi Klasifikasi Sampah/Dataset sampah sendiri/garbage_classification_sendiri/test'\n",
    "\n",
    "# Data transformations and augmentations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom Dataset class to handle corrupted images and transparent images\n",
    "class VerifyImageFolder(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            path, target = self.samples[index]\n",
    "            sample = self.loader(path)\n",
    "            \n",
    "            # Handle images with transparency (P or RGBA mode)\n",
    "            if sample.mode in ('P', 'RGBA'):\n",
    "                sample = sample.convert('RGBA').convert('RGB')\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                sample = self.transform(sample)\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "            \n",
    "            return sample, target\n",
    "        \n",
    "        except (OSError, IOError) as e:\n",
    "            print(f\"Error loading image at index {index}: {e}\")\n",
    "            return None  # Skip corrupted images\n",
    "\n",
    "# Custom collate function to filter out None items\n",
    "def custom_collate_fn(batch):\n",
    "    # Filter out None elements\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    \n",
    "    # If all items are None, return an empty batch\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    \n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "# Load datasets with custom class\n",
    "train_dataset = VerifyImageFolder(train_dir, transform=train_transforms)\n",
    "valid_dataset = VerifyImageFolder(valid_dir, transform=valid_transforms)\n",
    "test_dataset = VerifyImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "# DataLoaders for batch processing with custom collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Dapatkan mapping dari class index ke nama class\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Cetak mapping class index ke nama class\n",
    "print(\"Class index to name mapping:\")\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    print(f\"{idx}: {class_name}\")\n",
    "\n",
    "# Load pretrained EfficientNetB7 model from torchvision\n",
    "model = models.efficientnet_b7(pretrained=True)\n",
    "\n",
    "# Freeze base model layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last few layers of the base model\n",
    "for param in list(model.parameters())[-10:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Tentukan jumlah kelas\n",
    "num_classes = 9  # Misalkan kita memiliki 9 kelas\n",
    "\n",
    "# Add custom classifier layer\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),  # Reduce dropout rate\n",
    "    nn.Linear(128, num_classes)\n",
    ")\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=100):\n",
    "    best_acc = 0.0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for batch in train_loader:\n",
    "            # Skip empty batches\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                # Skip empty batches\n",
    "                if batch is None:\n",
    "                    continue\n",
    "                \n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(valid_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(valid_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc.item())\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model_sendiri_output_test.pth')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f'Best Val Acc: {best_acc:.4f}')\n",
    "    \n",
    "    # Plotting training and validation loss and accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Acc')\n",
    "    plt.plot(val_accuracies, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=100)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model_sendiri_output_test.pth'))\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_corrects = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Skip empty batches\n",
    "        if batch is None:\n",
    "            continue\n",
    "            \n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_prob.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "test_acc = test_corrects.double() / len(test_loader.dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# Plot confusion matrix dengan label nama class\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model performance using various metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')  \n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e51c8-90dc-4dc8-8114-e7c407101549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import tkinter as tk\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the trained model\n",
    "model_path = r'C:\\Users\\asus\\Downloads\\Testing Simulasi Klasifikasi Sampah\\model_complete_garbage_classification_9_sendiri_output.pth'\n",
    "model = torch.load(model_path)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define labels\n",
    "labels = {\n",
    "    0: 'anorganik-brown-glass',\n",
    "    1: 'anorganik-cardboard',\n",
    "    2: 'anorganik-green-glass',\n",
    "    3: 'anorganik-metal',\n",
    "    4: 'anorganik-paper',\n",
    "    5: 'anorganik-plastic',\n",
    "    6: 'anorganik-trash',\n",
    "    7: 'anorganik-white-glass',\n",
    "    8: 'organik-biological',\n",
    "    9: 'no-garbage'  # Add a label for no garbage\n",
    "}\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Traffic Light and Trash Bin Animation Class\n",
    "class TrafficLight(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title(\"Trash Bin Simulation\")\n",
    "        self.geometry(\"400x450\")  # Adjusted the width to fit the trash bins\n",
    "        \n",
    "        # Create Canvas for traffic light\n",
    "        self.canvas = tk.Canvas(self, width=200, height=400, bg='black')\n",
    "        self.canvas.pack(side=\"left\")\n",
    "\n",
    "        # Create Circles (Lamps)\n",
    "        self.red_light = self.canvas.create_oval(50, 50, 150, 150, fill=\"gray\")\n",
    "        self.yellow_light = self.canvas.create_oval(50, 150, 150, 250, fill=\"gray\")\n",
    "        self.green_light = self.canvas.create_oval(50, 250, 150, 350, fill=\"gray\")\n",
    "        \n",
    "        # Create Canvas for trash bins\n",
    "        self.trash_canvas = tk.Canvas(self, width=200, height=400, bg='white')\n",
    "        self.trash_canvas.pack(side=\"right\")\n",
    "\n",
    "        # Draw trash bins (closed state)\n",
    "        self.red_trash_bin = self.trash_canvas.create_rectangle(50, 50, 150, 150, fill=\"red\")\n",
    "        self.yellow_trash_bin = self.trash_canvas.create_rectangle(50, 150, 150, 250, fill=\"yellow\")\n",
    "        self.green_trash_bin = self.trash_canvas.create_rectangle(50, 250, 150, 350, fill=\"green\")\n",
    "\n",
    "        # Create Label for displaying the type of waste\n",
    "        self.label = tk.Label(self, text=\"\", font=(\"Helvetica\", 12))\n",
    "        self.label.pack(pady=10)\n",
    "\n",
    "        # Timer variables\n",
    "        self.prediction_timer = None\n",
    "        self.last_prediction = None\n",
    "\n",
    "    def red_on(self):\n",
    "        self.all_off()\n",
    "        self.canvas.itemconfig(self.red_light, fill=\"red\")\n",
    "        self.label.config(text=\"Sampah anorganik non-recycle\")\n",
    "        self.open_trash_bin('red')\n",
    "\n",
    "    def yellow_on(self):\n",
    "        self.all_off()\n",
    "        self.canvas.itemconfig(self.yellow_light, fill=\"yellow\")\n",
    "        self.label.config(text=\"Sampah anorganik recycle\")\n",
    "        self.open_trash_bin('yellow')\n",
    "\n",
    "    def green_on(self):\n",
    "        self.all_off()\n",
    "        self.canvas.itemconfig(self.green_light, fill=\"green\")\n",
    "        self.label.config(text=\"Sampah organik\")\n",
    "        self.open_trash_bin('green')\n",
    "\n",
    "    def all_off(self):\n",
    "        self.canvas.itemconfig(self.red_light, fill=\"gray\")\n",
    "        self.canvas.itemconfig(self.yellow_light, fill=\"gray\")\n",
    "        self.canvas.itemconfig(self.green_light, fill=\"gray\")\n",
    "        self.label.config(text=\"\")\n",
    "        self.close_all_bins()\n",
    "\n",
    "    def open_trash_bin(self, color):\n",
    "        if color == 'red':\n",
    "            self.trash_canvas.coords(self.red_trash_bin, 50, 50, 150, 120)  # Opening the red bin\n",
    "        elif color == 'yellow':\n",
    "            self.trash_canvas.coords(self.yellow_trash_bin, 50, 150, 150, 220)  # Opening the yellow bin\n",
    "        elif color == 'green':\n",
    "            self.trash_canvas.coords(self.green_trash_bin, 50, 250, 150, 320)  # Opening the green bin\n",
    "\n",
    "    def close_all_bins(self):\n",
    "        # Reset bins to closed state\n",
    "        self.trash_canvas.coords(self.red_trash_bin, 50, 50, 150, 150)\n",
    "        self.trash_canvas.coords(self.yellow_trash_bin, 50, 150, 150, 250)\n",
    "        self.trash_canvas.coords(self.green_trash_bin, 50, 250, 150, 350)\n",
    "\n",
    "# Class to handle \"No Garbage\" state\n",
    "class NoGarbage:\n",
    "    def __init__(self, traffic_light):\n",
    "        self.traffic_light = traffic_light\n",
    "\n",
    "    def execute(self):\n",
    "        self.traffic_light.all_off()  # Turn off all lights and close bins\n",
    "\n",
    "# Function to update the traffic light based on AI output\n",
    "def update_traffic_light(label, traffic_light):\n",
    "    if label == 'anorganik-trash':\n",
    "        traffic_light.red_on()\n",
    "    elif label == 'organik-biological':\n",
    "        traffic_light.green_on()\n",
    "    elif label == 'no-garbage':\n",
    "        no_garbage = NoGarbage(traffic_light)\n",
    "        no_garbage.execute()\n",
    "    else:\n",
    "        traffic_light.yellow_on()\n",
    "\n",
    "# Function to manage timer and ensure prediction consistency\n",
    "def check_prediction_consistency(label, traffic_light):\n",
    "    if label == 'no-garbage':\n",
    "        # Set the timer if 'no-garbage' is detected\n",
    "        if traffic_light.last_prediction != label:\n",
    "            traffic_light.last_prediction = label\n",
    "            traffic_light.prediction_timer = time.time()\n",
    "        elif time.time() - traffic_light.prediction_timer >= 2:  # 2 second timer\n",
    "            no_garbage = NoGarbage(traffic_light)\n",
    "            no_garbage.execute()\n",
    "    elif traffic_light.last_prediction is None:\n",
    "        traffic_light.last_prediction = label\n",
    "        traffic_light.prediction_timer = time.time()\n",
    "    elif label == traffic_light.last_prediction:\n",
    "        if time.time() - traffic_light.prediction_timer >= 2:  # 2 second timer\n",
    "            update_traffic_light(label, traffic_light)\n",
    "    else:\n",
    "        traffic_light.last_prediction = label\n",
    "        traffic_light.prediction_timer = time.time()\n",
    "\n",
    "# Function to run the waste classification and update GUI\n",
    "def run_classification():\n",
    "    # Open camera\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        exit()\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "        \n",
    "        # Convert frame to PIL Image\n",
    "        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Preprocess the frame for the model\n",
    "        img = transform(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Predict using the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            confidence, predicted = torch.max(probabilities, 1)\n",
    "            confidence = confidence.item()\n",
    "        \n",
    "        # Use a threshold for confidence\n",
    "        confidence_threshold = 0.6  # Adjust as needed\n",
    "        if confidence < confidence_threshold:\n",
    "            label = 'no-garbage'  # No garbage detected\n",
    "        else:\n",
    "            label = labels[predicted.item()]\n",
    "        \n",
    "        # Check prediction consistency with a 2-second delay\n",
    "        check_prediction_consistency(label, traffic_light)\n",
    "\n",
    "        # Display the resulting frame with the label and confidence\n",
    "        label_with_confidence = f\"{label}: {confidence * 100:.2f}%\"\n",
    "        cv2.putText(frame, label_with_confidence, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('Waste Classifier', frame)\n",
    "        \n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the camera and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create Traffic Light and Trash Bin GUI\n",
    "    traffic_light = TrafficLight()\n",
    "    \n",
    "    # Run the classification in a separate thread\n",
    "    Thread(target=run_classification).start()\n",
    "    \n",
    "    # Start the GUI loop\n",
    "    traffic_light.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
